import pandas as pd
import numpy as np

# Criando um dataframe ficticio
np.random.seed(42) # garante a reprodução do experimento
n = 10000 # Quantidade de clientes que queremos gerar dados ficticios

df = pd.DataFrame({
    "ID": range(1, n+1),
    "Idade": np.random.randint(18, 65, n),
    "Renda": np.random.randint(2000, 30000, n),
    "Regiao": np.random.choice(["Norte", "Sul", "Leste", "Oeste"], n)
})

df.sample(5)

## **Amostragem Aleatória Simples**

amostra_simples = df.sample(n=1000, random_state=42) # Random State tem o mesmo papel do seed.
amostra_simples.head()

## **Amostragem Aleatória Sistematica**

intervalo = np.random.randint(1, 50) # Gera intervalo com base na quantidade de registros arrendondando para baixo
amostra_sistematica = df.iloc[::intervalo, :]
amostra_sistematica.head()

## **Amostragem Estratificada**

from sklearn.model_selection import train_test_split

amostra_estratificada, _ = train_test_split(df, test_size=0.5, stratify=df["Regiao"])
amostra_estratificada.head()

## **Amostragem por Conglomerados**

clusters = df.groupby('Regiao')
amostra_conglomerados = clusters.get_group('Sul')
amostra_conglomerados.head()

## **Amostragem Por Conveniência**

amostra_conveniencia = df.head(1000)
amostra_conveniencia.head()

## **Amostragem por Julgamento**

amostra_julgamento = df[(df['Idade'] > 30) &
                        (df['Idade'] <=55) &
                        (df['Renda'] > 1500) |
                        (df['Regiao'] != 'Oeste')].sample(n=1000, random_state=42)

amostra_julgamento.head()

## **Amostragem por Cotas**

amostra_cotas = df.groupby('Regiao').apply(lambda x: x.sample(n=25)).reset_index(drop=True)
amostra_cotas.head()

## **Exercícios**

Comece executando a criação do conjunto de dados na célula abaixo e depois façam os exercícios abaixo:

1. Realizar uma amostragem aleatória simples com 500 registros.

2. Criar uma amostragem sistemática escolhendo cada 10º registro.

3. Dividir a base em estratos por localização e selecionar amostras proporcionais.

4. Selecionar aleatoriamente transações fraudulentas e comparar com transações não fraudulentas.

5. Criar um subconjunto de dados com base em amostragem por julgamento para transações acima de R$5000.

6. Aplicar amostragem por conglomerados dividindo os dados por tipo de transação e sorteando um grupo.

7. Executar uma amostragem por conveniência pegando os 300 primeiros registros.

8. Criar uma amostragem por cotas considerando o tipo de transação e localização.

9. Comparar os resultados das amostras aleatória e estratificada e explicar as diferenças.


import pandas as pd
import numpy as np

np.random.seed(42)
n = 5000  # Quantidade de registros

df = pd.DataFrame({
    'ID_Transacao': range(1, n+1),
    'Valor': np.random.uniform(10, 10000, n),
    'Tipo_Transacao': np.random.choice(['Compra', 'Transferencia', 'Pagamento'], n),
    'Localizacao': np.random.choice(['SP', 'RJ', 'MG', 'RS', 'BA', 'PR'], n),
    'Horario': np.random.choice(['Manha', 'Tarde', 'Noite', 'Madrugada'], n),
    'Fraude': np.random.choice([0, 1], n, p=[0.7, 0.3])
})
df.head()

